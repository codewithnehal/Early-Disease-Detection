{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb15e4c",
   "metadata": {},
   "source": [
    "# Part C: Early Disease Detection (Heart Disease)\n",
    "\n",
    "Classification task to predict **heart disease** presence (`disease` = 1/0). Includes EDA, preprocessing, Logistic Regression / Decision Tree / Random Forest, and evaluation with **Accuracy**, **Precision**, **Recall**, **F1**, **ROC-AUC**, and **Confusion Matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8690a",
   "metadata": {},
   "source": [
    "\n",
    "# Setup\n",
    "\n",
    "This notebook installs/imports commonly used libraries.  \n",
    "If some libraries are missing in your environment, run the install cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f59a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment to install dependencies in your environment\n",
    "# %pip install pandas numpy scikit-learn matplotlib seaborn joblib openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay)\n",
    "import joblib\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data_flex(path_or_url: str):\n",
    "    \"\"\"Load CSV/Excel from a local path or URL (Google Sheets/Drive direct link supported).\n",
    "    If it's a Google Sheet viewing URL, convert it to CSV export automatically.\n",
    "    \"\"\"\n",
    "    p = str(path_or_url).strip()\n",
    "    # Try to convert Google Sheets view link to export CSV\n",
    "    if \"docs.google.com/spreadsheets\" in p and \"/edit\" in p and \"export?format=csv\" not in p:\n",
    "        # Convert to CSV export\n",
    "        key = p.split(\"/d/\")[1].split(\"/\")[0]\n",
    "        p = f\"https://docs.google.com/spreadsheets/d/{key}/export?format=csv\"\n",
    "    try:\n",
    "        if p.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            df = pd.read_excel(p)\n",
    "        else:\n",
    "            df = pd.read_csv(p)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load data from '{path_or_url}'. Error: {e}\")\n",
    "    return df\n",
    "\n",
    "def quick_info(df: pd.DataFrame, name: str = \"Data\"):\n",
    "    print(f\"\\n{name} shape: {df.shape}\\n\")\n",
    "    display(df.head())\n",
    "    display(df.describe(include='all').transpose())\n",
    "    print(\"\\nMissing values per column:\\n\")\n",
    "    display(df.isna().sum().to_frame('missing'))\n",
    "    print(\"\\nDtypes:\\n\")\n",
    "    display(df.dtypes.to_frame('dtype'))\n",
    "\n",
    "def numeric_categorical_columns(df: pd.DataFrame, exclude_target: str = None):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    if exclude_target and exclude_target in num_cols:\n",
    "        num_cols.remove(exclude_target)\n",
    "    if exclude_target and exclude_target in cat_cols:\n",
    "        cat_cols.remove(exclude_target)\n",
    "    return num_cols, cat_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65460e3f",
   "metadata": {},
   "source": [
    "## 1) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === User input ===\n",
    "# Use either the Google Sheet link or local CSV/Excel path\n",
    "DATA_PATH_OR_URL = \"https://docs.google.com/spreadsheets/d/10k1aWfHrvoXJrHxMz6F2nfK5-UlkCmQ8EjhUt9Casso/edit?usp=sharing\"\n",
    "\n",
    "df = read_data_flex(DATA_PATH_OR_URL)\n",
    "quick_info(df, \"Raw Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6824b2",
   "metadata": {},
   "source": [
    "## 2) Target & EDA\n",
    "\n",
    "- Expected target column: **`disease`** (1 = has heart disease, 0 = no disease)\n",
    "- Basic distributions and class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90355fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = \"disease\"  # change if needed\n",
    "assert TARGET in df.columns, f\"Target column '{TARGET}' not found. Please update TARGET.\"\n",
    "\n",
    "# ensure binary numeric target\n",
    "if df[TARGET].dtype == 'object':\n",
    "    df[TARGET] = df[TARGET].astype(str).str.strip().str.lower().map({'yes':1, 'no':0, '1':1, '0':0})\n",
    "df[TARGET] = df[TARGET].astype(int)\n",
    "\n",
    "print(\"Class balance:\")\n",
    "display(df[TARGET].value_counts(normalize=True).to_frame('proportion'))\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if num_cols:\n",
    "    df[num_cols].hist(bins=30, figsize=(14, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92240272",
   "metadata": {},
   "source": [
    "## 3) Train/Test Split & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2136f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "num_cols, cat_cols = numeric_categorical_columns(X)\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_pipe, num_cols),\n",
    "    (\"cat\", categorical_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517afa1f",
   "metadata": {},
   "source": [
    "## 4) Train Models, Tune & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72474da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"LogisticRegression\": {\"model__C\": [0.1, 1.0, 10.0]},\n",
    "    \"DecisionTree\": {\"model__max_depth\": [3, 5, 10, None]},\n",
    "    \"RandomForest\": {\"model__n_estimators\": [100, 300], \"model__max_depth\": [None, 5, 10]}\n",
    "}\n",
    "\n",
    "eval_rows = []\n",
    "best_pipes = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess),\n",
    "                          (\"model\", model)])\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid[name], cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_pipe = grid.best_estimator_\n",
    "    best_pipes[name] = best_pipe\n",
    "    \n",
    "    y_pred = best_pipe.predict(X_test)\n",
    "    y_prob = best_pipe.predict_proba(X_test)[:, 1] if hasattr(best_pipe, \"predict_proba\") else None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan\n",
    "    \n",
    "    eval_rows.append([name, acc, prec, rec, f1, roc, grid.best_params_])\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC_AUC\", \"BestParams\"])\n",
    "display(eval_df.sort_values(\"F1\", ascending=False))\n",
    "\n",
    "best_name = eval_df.sort_values(\"F1\", ascending=False).iloc[0][\"Model\"]\n",
    "best_model = best_pipes[best_name]\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.title(f\"Confusion Matrix - {best_name}\")\n",
    "plt.show()\n",
    "\n",
    "# ROC\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    RocCurveDisplay.from_estimator(best_model, X_test, y_test)\n",
    "    plt.title(f\"ROC Curve - {best_name}\")\n",
    "    plt.show()\n",
    "\n",
    "joblib.dump(best_model, \"partC_best_model.joblib\")\n",
    "print(f\"Saved best Part C model as 'partC_best_model.joblib' ({best_name}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98ce08",
   "metadata": {},
   "source": [
    "## 5) Feature Importance / Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show top features for interpretability\n",
    "final_model = best_model.named_steps[\"model\"]\n",
    "pre = best_model.named_steps[\"preprocess\"]\n",
    "\n",
    "# Retrieve feature names after preprocessing\n",
    "oh = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"] if len(pre.transformers_)>1 else None\n",
    "num_cols, cat_cols = numeric_categorical_columns(df.drop(columns=[TARGET]))\n",
    "feat_names = []\n",
    "if num_cols:\n",
    "    feat_names.extend(num_cols)\n",
    "if cat_cols and oh is not None:\n",
    "    feat_names.extend(oh.get_feature_names_out(cat_cols).tolist())\n",
    "\n",
    "if hasattr(final_model, \"coef_\"):\n",
    "    coefs = pd.Series(final_model.coef_.ravel(), index=feat_names)\n",
    "    display(coefs.sort_values(ascending=False).head(15).to_frame(\"coef\"))\n",
    "    display(coefs.sort_values().head(15).to_frame(\"coef\"))\n",
    "elif hasattr(final_model, \"feature_importances_\"):\n",
    "    importances = pd.Series(final_model.feature_importances_, index=feat_names)\n",
    "    display(importances.sort_values(ascending=False).head(20).to_frame(\"importance\"))\n",
    "else:\n",
    "    print(\"Model does not expose coefficients/feature_importances.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d247dc3",
   "metadata": {},
   "source": [
    "## 6) Predict on New Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Predict on new data with same schema as training features\n",
    "# NEW_DATA_PATH = \"path/to/heart_new_data.csv\"\n",
    "# new_df = read_data_flex(NEW_DATA_PATH)\n",
    "# preds = best_model.predict(new_df)\n",
    "# out = new_df.copy()\n",
    "# out[\"disease_pred\"] = preds\n",
    "# out.to_csv(\"partC_predictions.csv\", index=False)\n",
    "# print(\"Saved predictions to 'partC_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa0097",
   "metadata": {},
   "source": [
    "## 7) Conclusions\n",
    "\n",
    "- Choose the model with highest F1/ROC-AUC.\n",
    "- Use feature insights to discuss the strongest correlates with disease."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
